## Embedding 
word2vec and glove are two most popular algorithm for word embedding. which maintain semantic similarity of words that captures different facets of the meaning of the word. 

## Main difference 
Word2vec embeddings are based on training a shallow feedforward neural network while glove embeddings are learnt based on matrix factorization techniques.
## Working Procedure
Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.
summary: word2vec suggested n-th words based on similar context. on the other hand golve shows the prbalilites two wods appear together. 

## word2vec
not very efficient to capture global information


## GloVe
it can capture global information